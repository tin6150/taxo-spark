#!/bin/sh


## script to submit spark job to uge cluster
## ref: http://web.global/apps/confluence/pages/viewpage.action?pageId=62031175
##


##   qsub ./uge_spark.qsub

#$   -N df-tid-pe320
#$   -pe spark-hadoop 320 
###$   -N df-rpt-pe4
###$   -l m_mem_free=10G                  # for 822 cores, can't get 32GB.  10 GB was fine, maybe can do 16
###$   -l h_rt=2190                   #  7200 = 2 hours
###$ -l h_rt=28100                  # 28100 = a tad less than 8 hr
###$ -l h_rt=281001                  # 281001 = a tad less than 78 hr
#$ -l h_rt=2810012                  # 281001 = a tad less than 780 hr
###$ -wd /path/to/workdir
###$ -o ./sprk-pe8.out              # don't use, cuz then combine .po and .o and breaks grep -v ^17
#$   -j y
#$   -cwd 
#$   -S /bin/sh
#$   -R y                               # use reservation, or large pe job may starve  


# This is a default way to determine Spark master port and host. Port is stored in the $TMPDIR/spark_port file . Host is stored in the $TMPDIR/spark_host file. The MASTER environment variable must be set to spark://${SPARK_HOST}:${SPARK_PORT}. You may also use this Spark URL in your own scripts if needed.
SPARK_PORT=`cat $TMPDIR/spark_port`;
SPARK_HOST=`cat $TMPDIR/spark_host`;
export MASTER=spark://${SPARK_HOST}:${SPARK_PORT}
# This string is not needed if your environment is set up correctly. We just put this line here to make sure we have the proper path for the modules.
export MODULEPATH=/prog/modules/all:$MODULEPATH
# To make Spark binaries available - we should load the Spark module. After this $SPARK_HOME variable will be set.
module load spark
#cd ${SPARK_HOME} ## ie /usr/prog/spark/1.6.0-hadoop-2.6

# Setting SPARK_LOCAL_DIRS to /scratch. It's important not to put all temporary SPARK data into the global /tmp on the node. The directory in /scratch will be created automatically, you should not care about it.
export SPARK_LOCAL_DIRS="/scratch/$USER/spark/${JOB_ID}.${SGE_TASK_ID}/tmp_local"
# run-example will submit the job to the Spark cluster using $MASTER variable. If $MASTER variable is not set - the job will be failed.
#  ./bin/run-example SparkPi 1000 &> /home/${USER}/script.out.t${JOB_ID}

##export PYTHONPATH=$PYTHONPATH:/prj/idinfo/prog/local_python_2.7.9/lib/python2.7/
#export PYTHONPATH=$PYTHONPATH:/home/bofh1/code/hoti-bb/taxo-spark/pyphy:/home/bofh1/local_python_2.7.9/lib/python2.7/site-packages/:/prj/idinfo/prog/local_python_2.7.9/lib/python2.7/
source /prj/idinfo/prog/local_python_2.7.9/bin/activate
#export PYTHONPATH=$PYTHONPATH:/home/bofh1/code/hoti-bb/taxo-spark:/prj/idinfo/prog/local_python_2.7.9/lib/python2.7
# pythonpath here don't seems to cut it for UDF, needed to use sparkConf setExecutorEnv() for that
# and don't seems to need it outside spark env... just need to use import correctly?  (or cuz i had -cwd in qsub?)



#spark-submit taxorpt_spark.py /home/bofh1/code/svn/taxo/db/protein_blast_hits.txt.head100 /home/bofh1/pub/taxorpt_uge_spark.htm  # tppl 20 min 
#spark-submit  taxoTraceTblOop.py 
#spark-submit taxorpt_df.py   ## /home/bofh1/code/svn/taxo/db/protein_blast_hits.txt.head100 /home/bofh1/pub/taxorpt_uge_spark.htm
        # 37 min with 8 core, 35 min with 48 core.  no sqlQuery/collect.  just 6 col addition
        # 96 min with 48 cores x 24 GB, with 3 sqlQuery/collect.  just 6 col addition
        # ??      822 cores x 10 GB... contrast with load_trace.py which run the query on the parquet file

#spark-submit taxoTraceTbl_df.py   ##   long process!
spark-submit  taxoTraceTbl_df.py   ##  new join w/ acc2taxid list in dev
#spark-submit  taxorpt_df.py   # in dev again...
#spark-submit taxorpt_df.py  -dd -s'|' --text /home/bofh1/code/svn/taxo/db/protein_blast_hits.txt.head100 /home/bofh1/pub/taxorpt_dt.txt  # 
# -d


